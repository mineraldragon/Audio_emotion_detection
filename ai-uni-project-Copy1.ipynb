{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-02-28T19:07:35.002231Z",
     "iopub.status.busy": "2022-02-28T19:07:35.001693Z",
     "iopub.status.idle": "2022-02-28T19:07:44.635043Z",
     "shell.execute_reply": "2022-02-28T19:07:44.633853Z",
     "shell.execute_reply.started": "2022-02-28T19:07:35.002123Z"
    }
   },
   "outputs": [],
   "source": [
    "#This notebook was originally copied from https://www.kaggle.com/samiaimad/ai-uni-project. \n",
    "#This copy is to make changes of my own \n",
    "#using virtual environment VoiceRec1\n",
    "#Rogier Landman 2022\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import re\n",
    "import splitfolders\n",
    "import random\n",
    "import shutil\n",
    "from vad import VoiceActivityDetector\n",
    "\n",
    "# librosa is a Python library for analyzing audio and music.\n",
    "import librosa\n",
    "import librosa.display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import soundfile as sf\n",
    "import skimage\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# to play the audio files\n",
    "from IPython.display import Audio\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import wave\n",
    "import pylab\n",
    "from pathlib import Path\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPool2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Assigning Data Paths to Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T19:08:16.515756Z",
     "iopub.status.busy": "2022-02-28T19:08:16.515429Z",
     "iopub.status.idle": "2022-02-28T19:08:16.521288Z",
     "shell.execute_reply": "2022-02-28T19:08:16.520485Z",
     "shell.execute_reply.started": "2022-02-28T19:08:16.515722Z"
    }
   },
   "outputs": [],
   "source": [
    "Ravdess = \"/Users/rogierlandman/from_Samsung/Machine_learning_datasets/Ravdess_audio/audio_speech_actors_01-24\"\n",
    "CremaD = \"/Users/rogierlandman/from_Samsung/Machine_learning_datasets/Crema-D_dataset/AudioWAV\"\n",
    "Tess = \"/Users/rogierlandman/from_Samsung/Machine_learning_datasets/TESS Toronto emotional speech set data/TESS Toronto emotional speech set data\"\n",
    "Savee = \"/Users/rogierlandman/from_Samsung/Machine_learning_datasets/Savee_dataset/AudioData\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Ravdess-emotional-speech-audio data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T19:08:24.505542Z",
     "iopub.status.busy": "2022-02-28T19:08:24.505217Z",
     "iopub.status.idle": "2022-02-28T19:08:25.248968Z",
     "shell.execute_reply": "2022-02-28T19:08:25.247931Z",
     "shell.execute_reply.started": "2022-02-28T19:08:24.505510Z"
    }
   },
   "outputs": [],
   "source": [
    "ravdess=glob.glob(Ravdess + \"/*/*.wav\")\n",
    "\n",
    "emotion=[]\n",
    "path=[]\n",
    "gender=[]\n",
    "intensity=[]\n",
    "for i in ravdess:\n",
    "    #print(i)\n",
    "    isplit= i.split('.')[0]\n",
    "    isplit=isplit.split('-')\n",
    "    emotion.append(int(isplit[2]))\n",
    "    gend=int(isplit[6])\n",
    "    if gend%2==0:\n",
    "        gender.append('Female')\n",
    "    else:\n",
    "        gender.append('Male')\n",
    "    intensity.append(int(isplit[3]))\n",
    "    for sp in emotion:\n",
    "        if sp==\"1\":\n",
    "            intensity.append('Normal')\n",
    "    path.append(i)\n",
    "edf=pd.DataFrame(emotion, columns=['Emotions'])\n",
    "pdf=pd.DataFrame(path, columns=['Path'])\n",
    "gdf=pd.DataFrame(gender, columns=['Gender'])\n",
    "idf=pd.DataFrame(intensity, columns=['Intensity'])\n",
    "\n",
    "ravdess_df=pd.concat([edf, pdf, gdf, idf], axis=1)\n",
    "\n",
    "ravdess_df.Emotions.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\n",
    "ravdess_df.Intensity.replace({1:'Normal', 2:'Strong'}, inplace=True)\n",
    "ravdess_df.head()\n",
    "ravdess_df.isnull()\n",
    "ravdess_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cremad data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T19:08:41.078395Z",
     "iopub.status.busy": "2022-02-28T19:08:41.077158Z",
     "iopub.status.idle": "2022-02-28T19:08:41.865205Z",
     "shell.execute_reply": "2022-02-28T19:08:41.864178Z",
     "shell.execute_reply.started": "2022-02-28T19:08:41.078330Z"
    }
   },
   "outputs": [],
   "source": [
    "crema=glob.glob(CremaD + \"/*.wav\")\n",
    "\n",
    "emotion=[]\n",
    "path=[]\n",
    "gender=[]\n",
    "intensity=[]\n",
    "for f in crema:\n",
    "    part2= Path(f).stem\n",
    "    part2=part2.split('_')\n",
    "    #print(part2)\n",
    "    if int(part2[0])%2==0:\n",
    "        gender.append('Female')\n",
    "    else:\n",
    "        gender.append('Male')\n",
    "        \n",
    "    if part2[2] == 'SAD':\n",
    "        emotion.append('sad')\n",
    "    elif part2[2] == 'ANG':\n",
    "        emotion.append('angry')\n",
    "    elif part2[2] == 'DIS':\n",
    "        emotion.append('disgust')\n",
    "    elif part2[2] == 'FEA':\n",
    "        emotion.append('fear')\n",
    "    elif part2[2] == 'HAP':\n",
    "        emotion.append('happy')\n",
    "    elif part2[2] == 'NEU':\n",
    "        emotion.append('neutral')\n",
    "    else:\n",
    "        emotion.append('Unknown')\n",
    "        \n",
    "    if part2[3]=='HI':\n",
    "        intensity.append('High')\n",
    "    elif part2[3]=='LO':\n",
    "        intensity.append('Low')\n",
    "    elif part2[3]=='MD':\n",
    "        intensity.append('Medium')\n",
    "    elif part2[3]=='XX':\n",
    "        intensity.append('Unspecified')\n",
    "        \n",
    "    path.append(f)\n",
    "    \n",
    "edf=pd.DataFrame(emotion, columns=['Emotions'])\n",
    "pdf=pd.DataFrame(path, columns=['Path'])\n",
    "gdf=pd.DataFrame(gender, columns=['Gender'])\n",
    "idf=pd.DataFrame(intensity, columns=['Intensity'])\n",
    "\n",
    "crema_df=pd.concat([edf, pdf, gdf, idf], axis=1)\n",
    "crema_df.head()\n",
    "crema_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Toronto-emotional-speech-set-tess data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T19:08:50.795402Z",
     "iopub.status.busy": "2022-02-28T19:08:50.794613Z",
     "iopub.status.idle": "2022-02-28T19:08:51.361293Z",
     "shell.execute_reply": "2022-02-28T19:08:51.360402Z",
     "shell.execute_reply.started": "2022-02-28T19:08:50.795351Z"
    }
   },
   "outputs": [],
   "source": [
    "tess=glob.glob(Tess + \"/*/*.wav\")\n",
    "\n",
    "emotion=[]\n",
    "path=[]\n",
    "gender=[]\n",
    "intensity=[]\n",
    "\n",
    "for i in tess:\n",
    "    part2= Path(i).stem\n",
    "    fsplit=part2.split('_')[2]\n",
    "    if fsplit=='ps':\n",
    "        emotion.append('surprise')\n",
    "    else:\n",
    "        emotion.append(fsplit)\n",
    "    gender.append('Female')\n",
    "    path.append(i)\n",
    "    intensity.append('Unspecified')\n",
    "        \n",
    "edf=pd.DataFrame(emotion, columns=['Emotions'])\n",
    "pdf=pd.DataFrame(path, columns=['Path'])\n",
    "gdf=pd.DataFrame(gender, columns=['Gender'])\n",
    "idf=pd.DataFrame(intensity, columns=['Intensity'])\n",
    "\n",
    "tess_df=pd.concat([edf, pdf, gdf, idf], axis=1)\n",
    "tess_df.head()\n",
    "tess_df.isnull()\n",
    "tess_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Savee Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T19:09:04.987680Z",
     "iopub.status.busy": "2022-02-28T19:09:04.987379Z",
     "iopub.status.idle": "2022-02-28T19:09:05.093980Z",
     "shell.execute_reply": "2022-02-28T19:09:05.092873Z",
     "shell.execute_reply.started": "2022-02-28T19:09:04.987647Z"
    }
   },
   "outputs": [],
   "source": [
    "savee=glob.glob(Savee + \"/*/*.wav\")\n",
    "\n",
    "emotion=[]\n",
    "path=[]\n",
    "gender=[]\n",
    "intensity=[]\n",
    "\n",
    "for f in savee:\n",
    "    #dsplit = d.split(\".\")[0]\n",
    "    #llsplit=re.split('\\d+',dsplit[0])\n",
    "    #print(llsplit)\n",
    "    part2= Path(f).stem\n",
    "    elist=part2[0]\n",
    "    if elist=='a':\n",
    "        emotion.append('angry')\n",
    "    elif elist=='d':\n",
    "        emotion.append('disgust')\n",
    "    elif elist=='f':\n",
    "        emotion.append('fear')\n",
    "    elif elist=='h':\n",
    "        emotion.append('happy')\n",
    "    elif elist=='n':\n",
    "        emotion.append('neutral')\n",
    "    elif elist=='sa':\n",
    "        emotion.append('sad')\n",
    "    else:\n",
    "        emotion.append('surprise')\n",
    "    gender.append('Male')\n",
    "    path.append(f)\n",
    "    intensity.append('Unspecified')\n",
    "\n",
    "edf=pd.DataFrame(emotion, columns=['Emotions'])\n",
    "pdf=pd.DataFrame(path, columns=['Path'])\n",
    "gdf=pd.DataFrame(gender, columns=['Gender'])\n",
    "idf=pd.DataFrame(intensity, columns=['Intensity'])\n",
    "\n",
    "savee_df=pd.concat([edf, pdf, gdf, idf], axis=1)\n",
    "#savee_df.head()\n",
    "savee_df.isnull()\n",
    "savee_df.info()\n",
    "savee_df.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Combining DataFrames (ravdess_df, crema_df,tess_df, savee_df)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T19:09:26.339795Z",
     "iopub.status.busy": "2022-02-28T19:09:26.339380Z",
     "iopub.status.idle": "2022-02-28T19:09:26.372158Z",
     "shell.execute_reply": "2022-02-28T19:09:26.370986Z",
     "shell.execute_reply.started": "2022-02-28T19:09:26.339761Z"
    }
   },
   "outputs": [],
   "source": [
    "Data_Combined = pd.concat([ravdess_df, crema_df,tess_df, savee_df], axis = 0, ignore_index = True)\n",
    "Data_Combined.isnull()\n",
    "Data_Combined.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T19:09:56.038650Z",
     "iopub.status.busy": "2022-02-28T19:09:56.038276Z",
     "iopub.status.idle": "2022-02-28T19:10:02.462663Z",
     "shell.execute_reply": "2022-02-28T19:10:02.461132Z",
     "shell.execute_reply.started": "2022-02-28T19:09:56.038611Z"
    }
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "flag = 0\n",
    "\n",
    "while (flag != 2000): \n",
    "   \n",
    "    audio = wave.open(Data_Combined.iloc[flag, 1], 'r')\n",
    "    flag = flag + 1 \n",
    "    y= Data_Combined.iloc[flag, 0]\n",
    "    if (len(x)>10):\n",
    "        flag=2000    \n",
    "\n",
    "    if (y not in x):\n",
    "        signal = np.frombuffer(audio.readframes(-1), dtype=np.int16)\n",
    "        plt.figure(figsize=(12,12))\n",
    "        plot1 = plt.subplot(211)\n",
    "        plot1.set_title(Data_Combined.iloc[flag, 0])\n",
    "        plot1.plot(signal)\n",
    "        plot1.set_xlabel('time * Number of samples')\n",
    "        plot1.set_ylabel('energy')\n",
    "\n",
    "        plot2 = plt.subplot(212)\n",
    "        plot2.specgram(signal, NFFT=1024, Fs=12281, noverlap=900)\n",
    "        plot2.set_xlabel('Time')\n",
    "        plot2.set_ylabel('Frequency')\n",
    "        \n",
    "        x.append(y)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T19:10:33.779416Z",
     "iopub.status.busy": "2022-02-28T19:10:33.779079Z",
     "iopub.status.idle": "2022-02-28T19:10:33.785429Z",
     "shell.execute_reply": "2022-02-28T19:10:33.784016Z",
     "shell.execute_reply.started": "2022-02-28T19:10:33.779382Z"
    }
   },
   "outputs": [],
   "source": [
    "def stretch(data, rate):\n",
    "    return librosa.effects.time_stretch(data, rate)\n",
    "\n",
    "#x , sr = librosa.load('/Users/rogierlandman/from_Samsung/Machine_learning_datasets/Crema-D_dataset/AudioWAV/1001_DFA_ANG_XX.wav')\n",
    "#ya = stretch(x, 1.05)\n",
    "#librosa.display.waveshow(y=x, sr=sr)\n",
    "#librosa.display.waveshow(y=ya, sr=sr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T19:10:39.754942Z",
     "iopub.status.busy": "2022-02-28T19:10:39.754615Z",
     "iopub.status.idle": "2022-02-28T19:10:40.160811Z",
     "shell.execute_reply": "2022-02-28T19:10:40.159129Z",
     "shell.execute_reply.started": "2022-02-28T19:10:39.754910Z"
    }
   },
   "outputs": [],
   "source": [
    "def white_noise(data,SNR):\n",
    "    RMS_s=math.sqrt(np.mean(data**2))\n",
    "    RMS_n=math.sqrt(RMS_s**2/(pow(10,SNR/10)))\n",
    "    STD_n=RMS_n\n",
    "    noise=np.random.normal(0, STD_n, data.shape[0])\n",
    "    data_noise=noise+data\n",
    "    return data_noise \n",
    "\n",
    "#x , sr = librosa.load('/Users/rogierlandman/from_Samsung/Machine_learning_datasets/Crema-D_dataset/AudioWAV/1001_DFA_ANG_XX.wav')\n",
    "#ya = white_noise(x, 20)\n",
    "#librosa.display.waveshow(y=x, sr=sr)\n",
    "#librosa.display.waveshow(y=ya, sr=sr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-01-19T09:27:53.322418Z",
     "iopub.status.idle": "2022-01-19T09:27:53.323563Z",
     "shell.execute_reply": "2022-01-19T09:27:53.323314Z",
     "shell.execute_reply.started": "2022-01-19T09:27:53.323255Z"
    }
   },
   "outputs": [],
   "source": [
    "def shifting_time(data, sampling_rate, shift_max, shift_direction):\n",
    "    shift = np.random.randint(sampling_rate * shift_max)\n",
    "    if shift_direction == 'right':\n",
    "        shift = -shift\n",
    "    elif shift_direction == 'both':\n",
    "        direction = np.random.randint(0, 2)\n",
    "        if direction == 1:\n",
    "            shift = -shift\n",
    "    augmented_data = np.roll(data, shift)\n",
    "    # Set to silence for heading/ tailing\n",
    "    #if shift > 0:\n",
    "    #    augmented_data[:shift] = 0\n",
    "    #else:\n",
    "    #    augmented_data[shift:] = 0\n",
    "    return augmented_data\n",
    "\n",
    "#x , sr = librosa.load('/Users/rogierlandman/from_Samsung/Machine_learning_datasets/Crema-D_dataset/AudioWAV/1001_DFA_ANG_XX.wav')\n",
    "#ya = shifting_time(x, 16000, 0.5, 'right')\n",
    "#librosa.display.waveshow(y=x, sr=sr)\n",
    "#librosa.display.waveshow(y=ya, sr=sr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-01-19T09:27:53.324873Z",
     "iopub.status.idle": "2022-01-19T09:27:53.325416Z",
     "shell.execute_reply": "2022-01-19T09:27:53.325215Z",
     "shell.execute_reply.started": "2022-01-19T09:27:53.325194Z"
    }
   },
   "outputs": [],
   "source": [
    "def pitch(data, sampling_rate, pitch_factor):\n",
    "    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)\n",
    "#x , sr = librosa.load(audio_data)\n",
    "#ya = pitch(x,sr,pitch_factor=0.5)\n",
    "#plt.figure(figsize=(14,4))\n",
    "#librosa.display.waveplot(y=ya, sr=sr)\n",
    "#Audio(ya, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Spectograms images from Audio files (old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T19:12:01.018064Z",
     "iopub.status.busy": "2022-02-28T19:12:01.017775Z"
    }
   },
   "outputs": [],
   "source": [
    "bad_ones = []\n",
    "\n",
    "output_path=\"/Users/rogierlandman/from_Samsung/Machine_learning_datasets/temp/\"\n",
    "if  not os.path.exists(os.path.join(output_path, 'audio-spectrograms')):\n",
    "    os.mkdir(os.path.join(output_path, 'audio-spectrograms'))\n",
    "    \n",
    "#function to get sound and frame rate info\n",
    "def get_audio_info(audio_path):\n",
    "    frame_rate, audio_info = wavfile.read(audio_path)\n",
    "    return audio_info, frame_rate\n",
    "\n",
    "for f in range (len(Data_Combined)):\n",
    "        file_Path = f'{Data_Combined.iloc[f, 1]}'\n",
    "        #print (file_path)\n",
    "        file_stem = Path(file_Path).stem\n",
    "        #print (file_stem)\n",
    "        target_dir = f'class_{Data_Combined.iloc[f, 0]}'\n",
    "        #print (target_dir)\n",
    "        dist_dir = os.path.join(os.path.join(output_path, 'audio-spectrograms'), target_dir)\n",
    "        #print (dist_dir)\n",
    "        # ex: ./audio-spectrograms/class_fear\n",
    "        file_dist_path = os.path.join(dist_dir, file_stem)\n",
    "        #ex: ./audio-spectrograms/class_fear/03-01-06-02-02-01-10\n",
    "        \n",
    "    \n",
    "        if not os.path.exists(file_dist_path + '.png'):\n",
    "            if not os.path.exists(dist_dir):\n",
    "                os.mkdir(dist_dir)\n",
    "            file_stem = Path(file_Path).stem\n",
    "            try:\n",
    "                audio_info, frame_rate = get_audio_info(file_Path)\n",
    "                pylab.specgram(audio_info, Fs=frame_rate)\n",
    "                pylab.savefig(f'{file_dist_path}.png')\n",
    "                pylab.close()\n",
    "                print(file_Path)\n",
    "            except:\n",
    "                bad_ones.append(file_Path)\n",
    "                print('BAD: ' + file_Path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectrograms for augmented audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T19:12:01.018064Z",
     "iopub.status.busy": "2022-02-28T19:12:01.017775Z"
    }
   },
   "outputs": [],
   "source": [
    "#replicate folder structure\n",
    "\n",
    "input_path=\"/Users/rogierlandman/from_Samsung/Machine_learning_datasets/augment/audio/\"\n",
    "output_path=\"/Users/rogierlandman/from_Samsung/Machine_learning_datasets/augment/audio-spectrograms/\"\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(input_path):\n",
    "    structure = os.path.join(output_path, dirpath[len(input_path):])\n",
    "    if not os.path.isdir(structure):\n",
    "        os.mkdir(structure)\n",
    "    else:\n",
    "        print(\"Folder already exists!\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T19:12:01.018064Z",
     "iopub.status.busy": "2022-02-28T19:12:01.017775Z"
    }
   },
   "outputs": [],
   "source": [
    "bad_ones = []\n",
    "\n",
    "#make spectrograms\n",
    "\n",
    "input_path=\"/Users/rogierlandman/from_Samsung/Machine_learning_datasets/augment/audio/\"\n",
    "output_path=\"/Users/rogierlandman/from_Samsung/Machine_learning_datasets/augment/audio-spectrograms/\"\n",
    "\n",
    "#shutil.rmtree(os.path.join(output_path,'class_angry'))\n",
    "#os.makedirs(os.path.join(output_path,'class_angry'))\n",
    "#shutil.rmtree(os.path.join(output_path,'class_disgust'))\n",
    "#os.makedirs(os.path.join(output_path,'class_disgust'))\n",
    "#shutil.rmtree(os.path.join(output_path,'class_fear'))\n",
    "#os.makedirs(os.path.join(output_path,'class_fear'))\n",
    "#shutil.rmtree(os.path.join(output_path,'class_happy'))\n",
    "#os.makedirs(os.path.join(output_path,'class_happy'))\n",
    "#shutil.rmtree(os.path.join(output_path,'class_neutral'))\n",
    "#os.makedirs(os.path.join(output_path,'class_neutral'))\n",
    "#shutil.rmtree(os.path.join(output_path,'class_sad'))\n",
    "#os.makedirs(os.path.join(output_path,'class_sad'))\n",
    "#shutil.rmtree(os.path.join(output_path,'class_surprise'))\n",
    "#os.makedirs(os.path.join(output_path,'class_surprise'))\n",
    "\n",
    "\n",
    "#function to get sound and frame rate info\n",
    "def get_audio_info(audio_path):\n",
    "    wav = wave.open(audio_path, 'r')\n",
    "    \n",
    "    frames = wav.readframes(-1)\n",
    "    audio_info = frames, 'int16'\n",
    "    frame_rate = wav.getframerate()\n",
    "    wav.close()\n",
    "    return audio_info, frame_rate\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(input_path):\n",
    "    for file in filenames:\n",
    "        if (file[-3:] == 'wav'):\n",
    "            tmp = os.path.join(dirpath,file)\n",
    "            file_Path=Path(tmp)\n",
    "            file_stem = Path(file_Path).stem\n",
    "            class_dir = file_Path.parts[-2]\n",
    "            new_path = os.path.join(output_path, class_dir, file_stem)                                 \n",
    "\n",
    "            if not os.path.exists(new_path + '.png'):\n",
    "                print(str(file_Path))\n",
    "                if not file_Path == '/Users/rogierlandman/from_Samsung/Machine_learning_datasets/augment/audio/class_fear/OAF_long_fear_aug1.wav':\n",
    "                    try:\n",
    "                        audio_info, frame_rate = get_audio_info(str(file_Path))\n",
    "                        pylab.specgram(audio_info, Fs=frame_rate)\n",
    "                        pylab.savefig(f'{new_path}.png')\n",
    "                        pylab.close()\n",
    "                        #print(file_Path)\n",
    "                    except:\n",
    "                        bad_ones.append(file_Path)\n",
    "                        print('BAD: ' + file_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T19:12:01.018064Z",
     "iopub.status.busy": "2022-02-28T19:12:01.017775Z"
    }
   },
   "outputs": [],
   "source": [
    "bad_ones = []\n",
    "\n",
    "#make spectrograms\n",
    "\n",
    "input_path=\"/Users/rogierlandman/from_Samsung/Machine_learning_datasets/augment/audio/\"\n",
    "output_path=\"/Users/rogierlandman/from_Samsung/Machine_learning_datasets/augment/audio-spectrograms/\"\n",
    "\n",
    "shutil.rmtree(os.path.join(output_path,'class_angry'))\n",
    "os.makedirs(os.path.join(output_path,'class_angry'))\n",
    "shutil.rmtree(os.path.join(output_path,'class_disgust'))\n",
    "os.makedirs(os.path.join(output_path,'class_disgust'))\n",
    "shutil.rmtree(os.path.join(output_path,'class_fear'))\n",
    "os.makedirs(os.path.join(output_path,'class_fear'))\n",
    "shutil.rmtree(os.path.join(output_path,'class_happy'))\n",
    "os.makedirs(os.path.join(output_path,'class_happy'))\n",
    "shutil.rmtree(os.path.join(output_path,'class_neutral'))\n",
    "os.makedirs(os.path.join(output_path,'class_neutral'))\n",
    "shutil.rmtree(os.path.join(output_path,'class_sad'))\n",
    "os.makedirs(os.path.join(output_path,'class_sad'))\n",
    "shutil.rmtree(os.path.join(output_path,'class_surprise'))\n",
    "os.makedirs(os.path.join(output_path,'class_surprise'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T19:12:01.018064Z",
     "iopub.status.busy": "2022-02-28T19:12:01.017775Z"
    }
   },
   "outputs": [],
   "source": [
    "input_path=\"/Users/rogierlandman/from_Samsung/Machine_learning_datasets/augment/audio/\"\n",
    "output_path=\"/Users/rogierlandman/from_Samsung/Machine_learning_datasets/augment/audio-spectrograms/\"\n",
    "\n",
    "#function to get sound and frame rate info\n",
    "def get_audio_info(audio_path):\n",
    "    wav = wave.open(audio_path, 'r')\n",
    "    frames = wav.readframes(-1)\n",
    "    audio_info = pylab.frombuffer(frames, 'int16')\n",
    "    frame_rate = wav.getframerate()\n",
    "    wav.close()\n",
    "    return audio_info, frame_rate\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(input_path):\n",
    "    for file in filenames:\n",
    "        if (file[-3:] == 'wav'):\n",
    "            tmp = os.path.join(dirpath,file)\n",
    "            file_Path=Path(tmp)\n",
    "            file_stem = Path(file_Path).stem\n",
    "            class_dir = file_Path.parts[-2]\n",
    "            new_path = os.path.join(output_path, class_dir, file_stem)                                 \n",
    "\n",
    "            if not os.path.exists(new_path + '.png'):\n",
    "                try:\n",
    "                    audio_info, frame_rate = get_audio_info(str(file_Path))\n",
    "                    pylab.specgram(audio_info, Fs=frame_rate)\n",
    "                    pylab.savefig(f'{new_path}.png')\n",
    "                    pylab.close()\n",
    "                    print(file_Path)\n",
    "                except:\n",
    "                    bad_ones.append(file_Path)\n",
    "                    print('BAD: ' + file_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(file_Path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking duration of audio samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T19:12:01.018064Z",
     "iopub.status.busy": "2022-02-28T19:12:01.017775Z"
    }
   },
   "outputs": [],
   "source": [
    "output_path=\"/Users/rogierlandman/from_Samsung/Machine_learning_datasets/temp1/\"\n",
    "    \n",
    "duration = []\n",
    "\n",
    "for f in range (len(Data_Combined)):\n",
    "    \n",
    "    file_Path = f'{Data_Combined.iloc[f, 1]}'\n",
    "\n",
    "    #load audio ..\n",
    "    data, samplerate = sf.read(file_Path)\n",
    "\n",
    "    #resample\n",
    "    duration.append(len(data) / samplerate)\n",
    "\n",
    "plt.hist(duration)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that there is quite a bit of variation in the length of audio samples and there are considerable periods of no signal (quiet) in the audio. \n",
    "\n",
    "It makes sense to try a different approach, which is to put all samples of one category together, remove quiet parts, and then re-segment into pieces of a single length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New approach\n",
    "\n",
    "This did not turn out as good as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T19:12:01.018064Z",
     "iopub.status.busy": "2022-02-28T19:12:01.017775Z"
    }
   },
   "outputs": [],
   "source": [
    "def scale_minmax(X, min=0.0, max=1.0):\n",
    "    X_std = (X - X.min()) / (X.max() - X.min())\n",
    "    X_scaled = X_std * (max - min) + min\n",
    "    return X_scaled\n",
    "\n",
    "def spectrogram_image(y, sr, out, fft_length, hop_length, n_mels):\n",
    "    # use log-melspectrogram\n",
    "    mels = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, n_fft=fft_length, hop_length=hop_length)\n",
    "    mels = np.log(mels + 1e-9) # add small number to avoid log(0)\n",
    "\n",
    "    # min-max scale to fit inside 8-bit range\n",
    "    img = scale_minmax(mels, 0, 255).astype(np.uint8)\n",
    "    img = np.flip(img, axis=0) # put low frequencies at the bottom in image\n",
    "    img = 255-img # invert. make black==more energy\n",
    "\n",
    "    # save as PNG\n",
    "    skimage.io.imsave(out, img)\n",
    "\n",
    "\n",
    "output_path=\"/Users/rogierlandman/from_Samsung/Machine_learning_datasets/temp1/\"\n",
    "\n",
    "target_samplerate = 22050\n",
    "hop_length = 256 # number of samples per time-step in spectrogram\n",
    "fft_length = 1024 #length of fft\n",
    "n_mels = 128 # number of bins in spectrogram. Height of image\n",
    "#desired segment length in seconds\n",
    "segment_length = 2\n",
    "segment_length_samp = np.round(segment_length * target_samplerate)  \n",
    "\n",
    "labels = ['neutral','angry','disgust','fear','happy','sad','surprise']\n",
    "#labels = ['angry','disgust','fear','happy','sad','surprise']\n",
    "   \n",
    "for label in labels:\n",
    "    \n",
    "    print(label, ' accumulate data')\n",
    "    lst = []\n",
    "    cumdata = np.array(lst)\n",
    "    for f in range (len(Data_Combined)):\n",
    "    #    for f in range (3):\n",
    "\n",
    "        #print(str(f), 'of ', str(len(Data_Combined)))\n",
    "        file_Path = f'{Data_Combined.iloc[f, 1]}'\n",
    "        #file_stem = Path(file_Path).stem\n",
    "        #print(Data_Combined.iloc[f, 0])\n",
    "        tmp = Data_Combined.iloc[f, 0]\n",
    "        if(tmp == label):  \n",
    "            \n",
    "            try:\n",
    "\n",
    "                v = VoiceActivityDetector(file_Path)\n",
    "                tmp = v.detect_speech()\n",
    "\n",
    "                z = tmp.shape[0]\n",
    "\n",
    "                new_data = np.array(lst)\n",
    "\n",
    "                for i in range(0, z-1):\n",
    "                    if(tmp[i,1] == 1):\n",
    "\n",
    "                        a=int(tmp[i+1,0])\n",
    "                        b=int(tmp[i,0])\n",
    "                        data=v.data[b:a]                 \n",
    "                        new_data = np.concatenate([new_data,data])\n",
    "\n",
    "                if(new_data.size>1):        \n",
    "                    new_data=new_data.astype(float)\n",
    "                    new_data = librosa.resample(new_data, orig_sr=v.rate, target_sr=target_samplerate)\n",
    "                    c_mx = np.max(new_data) \n",
    "                    c_mn = np.min(new_data)\n",
    "                    new_data = 2 * ((new_data - c_mx) / (c_mx - c_mn)) + 1 \n",
    "\n",
    "                cumdata = np.concatenate([cumdata,new_data])\n",
    "                \n",
    "            except:\n",
    "                \n",
    "                print('Bad: ', file_Path )\n",
    "\n",
    "                \n",
    "    print(label, ' spectrograms')\n",
    "    \n",
    "    #chop into pieces\n",
    "    n_pieces = 0\n",
    "    labelstring = 'class_' + label\n",
    "    if  not os.path.exists(os.path.join(output_path,  'audio-spectrograms', labelstring)):\n",
    "        os.mkdir(os.path.join(output_path, 'audio-spectrograms', labelstring))\n",
    "\n",
    "    while(len(cumdata)>segment_length_samp):\n",
    "        n_pieces += 1\n",
    "        tmp = cumdata[:segment_length_samp]\n",
    "\n",
    "        #make spectrogram ..\n",
    "        s1 = str(n_pieces).zfill(4)\n",
    "        s2 = 'sample_' + s1 + '.png'\n",
    "        \n",
    "        target_path = os.path.join(output_path, 'audio-spectrograms',labelstring, s2)\n",
    "        spectrogram_image(tmp, target_samplerate, target_path, fft_length, hop_length, n_mels)\n",
    "\n",
    "        #remove from big array ..\n",
    "        cumdata = np.delete(cumdata, np.s_[:segment_length_samp])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectrogram augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this would be done if you augment spectrograms \n",
    "#using frequency mask and time mask\n",
    "#https://towardsdatascience.com/audio-deep-learning-made-simple-part-3-data-preparation-and-augmentation-24c6e1f6b52\n",
    "\n",
    "def freq_mask(data, F):\n",
    "        \n",
    "        v = data.shape[1] # no. of bins       \n",
    "        # apply F frequency masks to the spectrogram\n",
    "        for i in range(F):\n",
    "            f = int(np.random.uniform(0, F)) # [0, F)\n",
    "            f0 = random.randint(0, v - f) # [0, v - f)\n",
    "            data[:, f0:f0 + f, :, :] = 0          \n",
    "        return data\n",
    "    \n",
    "    \n",
    "def time_mask(data, T):\n",
    "\n",
    "    tau = data.shape[2] # time points\n",
    "    # apply T time masks to the spectrogram\n",
    "    for i in range(T):\n",
    "        t = int(np.random.uniform(0, T)) # [0, T)\n",
    "        t0 = random.randint(0, tau - t) # [0, tau - t)\n",
    "        data[:, :, t0:t0 + t, :] = 0\n",
    "    return data\n",
    "    \n",
    "\n",
    "#replicate directory structure ..\n",
    "inputpath = '/Users/rogierlandman/from_Samsung/Machine_learning_datasets/temp/audio-spectrograms/'\n",
    "outputpath = '/Users/rogierlandman/from_Samsung/Machine_learning_datasets/augment/spectrograms/'\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(inputpath):\n",
    "    structure = os.path.join(outputpath, dirpath[len(inputpath):])\n",
    "    if not os.path.isdir(structure):\n",
    "        os.mkdir(structure)\n",
    "    else:\n",
    "        print(\"Folder already exists!\")\n",
    "\n",
    "        \n",
    "#loop through spectrogram files and augment .. \n",
    "for root, directories, file in os.walk(inputpath):\n",
    "    for file in file:\n",
    "        if(file.endswith(\".png\")):\n",
    "            #load the file\n",
    "            img = Image.open(file)\n",
    "            \n",
    "            #choose frequency or time mask\n",
    "            z = random.randint(0,1)\n",
    "            #apply\n",
    "            if(z):\n",
    "                img = freq_mask(data, 10)\n",
    "            else:\n",
    "                img = time_mask(data, 10)\n",
    "            \n",
    "            #save new image\n",
    "            file_stem = Path(file).stem\n",
    "            new_name = file_stem + '_aug1.png'\n",
    "            \n",
    "            tmp = os.path.join(root,file)\n",
    "            p=Path(tmp)\n",
    "            class_dir = p.parts[-2]\n",
    "            new_path = os.path.join(outputpath, class_dir, new_name)\n",
    "            \n",
    "            #to do: save as png image\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this would be done if you augment audio but then you need time to calculate spectrograms as well\n",
    "\n",
    "#loop through wav files and augment .. \n",
    "dest_dir = '/Users/rogierlandman/from_Samsung/Machine_learning_datasets/augment/audio/'\n",
    "\n",
    "shutil.rmtree(os.path.join(dest_dir,'class_angry'))\n",
    "os.makedirs(os.path.join(dest_dir,'class_angry'))\n",
    "shutil.rmtree(os.path.join(dest_dir,'class_disgust'))\n",
    "os.makedirs(os.path.join(dest_dir,'class_disgust'))\n",
    "shutil.rmtree(os.path.join(dest_dir,'class_fear'))\n",
    "os.makedirs(os.path.join(dest_dir,'class_fear'))\n",
    "shutil.rmtree(os.path.join(dest_dir,'class_happy'))\n",
    "os.makedirs(os.path.join(dest_dir,'class_happy'))\n",
    "shutil.rmtree(os.path.join(dest_dir,'class_neutral'))\n",
    "os.makedirs(os.path.join(dest_dir,'class_neutral'))\n",
    "shutil.rmtree(os.path.join(dest_dir,'class_sad'))\n",
    "os.makedirs(os.path.join(dest_dir,'class_sad'))\n",
    "shutil.rmtree(os.path.join(dest_dir,'class_surprise'))\n",
    "os.makedirs(os.path.join(dest_dir,'class_surprise'))\n",
    "\n",
    "for f in range (len(Data_Combined)):\n",
    " \n",
    "    file_Path = f'{Data_Combined.iloc[f, 1]}'\n",
    "    print(file_Path)\n",
    "    x, sr = librosa.load(file_Path)\n",
    "    file_stem = Path(file_Path).stem\n",
    "\n",
    "    R = random.randint(1,4)\n",
    "    if(R==1):\n",
    "    \n",
    "        #pick value for pitch shift\n",
    "        pick = random.uniform(-1,1)\n",
    "        x = pitch(x,sr, pitch_factor = pick)\n",
    "\n",
    "    if(R==2):\n",
    "    \n",
    "        #pick value for time shift\n",
    "        pick = random.uniform(0,0.5)\n",
    "        x = shifting_time(x,sr,shift_max=pick,shift_direction='both')\n",
    "\n",
    "    if(R==3):\n",
    "        \n",
    "        #pick value for white noise\n",
    "        pick = 20\n",
    "        x = white_noise(x,SNR=pick)\n",
    "\n",
    "    if(R==4):\n",
    "    \n",
    "        #pick value for time stretch\n",
    "        pick = random.uniform(0.95,1.05)\n",
    "        x = stretch(x, pick)\n",
    "\n",
    "    target_dir = f'class_{Data_Combined.iloc[f, 0]}'\n",
    "    dest_dir1 = os.path.join(dest_dir, target_dir)\n",
    "    dest_path = os.path.join(dest_dir1, file_stem + '_aug1.wav')\n",
    "\n",
    "    if not os.path.exists(dest_dir1):\n",
    "        os.mkdir(dest_dir1)\n",
    "\n",
    "\n",
    "    sf.write(dest_path, x, sr, 'PCM_16')\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-01-19T09:27:53.327644Z",
     "iopub.status.idle": "2022-01-19T09:27:53.328039Z",
     "shell.execute_reply": "2022-01-19T09:27:53.327885Z",
     "shell.execute_reply.started": "2022-01-19T09:27:53.327869Z"
    }
   },
   "outputs": [],
   "source": [
    "# Declare constants\n",
    "image_h = 128\n",
    "image_w = 128\n",
    "batch_size = 32\n",
    "num_channel = 3\n",
    "num_classes = 7\n",
    "INPUT_DIR = '/Users/rogierlandman/from_Samsung/Machine_learning_datasets/temp/audio-spectrograms/'\n",
    "OUTPUT_DIR = '/Users/rogierlandman/from_Samsung/Machine_learning_datasets/temp/output/'\n",
    "\n",
    "splitfolders.ratio(INPUT_DIR, output=OUTPUT_DIR,\n",
    "    seed=1337, ratio=(.8, .1, .1), group_prefix=None, move=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-01-19T09:27:53.327644Z",
     "iopub.status.idle": "2022-01-19T09:27:53.328039Z",
     "shell.execute_reply": "2022-01-19T09:27:53.327885Z",
     "shell.execute_reply.started": "2022-01-19T09:27:53.327869Z"
    }
   },
   "outputs": [],
   "source": [
    "# dataset training spectrograms\n",
    "training_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "                                             batch_size=batch_size,\n",
    "                                             validation_split=None,\n",
    "                                             directory=os.path.join(OUTPUT_DIR, 'train'),\n",
    "                                             shuffle=True,\n",
    "                                             color_mode='rgb',\n",
    "                                             image_size=(image_h, image_w),\n",
    "                                             subset=None,\n",
    "                                             seed=0)\n",
    "\n",
    "# dataset validation spectrograms\n",
    "validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "                                             batch_size=batch_size,\n",
    "                                             validation_split=None,\n",
    "                                             directory=os.path.join(OUTPUT_DIR, 'val'),\n",
    "                                             shuffle=True,\n",
    "                                             color_mode='rgb',\n",
    "                                             image_size=(image_h, image_w),\n",
    "                                             subset=None,\n",
    "                                             seed=0)\n",
    "\n",
    "# dataset test spectrograms\n",
    "test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "                                             batch_size=batch_size,\n",
    "                                             validation_split=None,\n",
    "                                             directory=os.path.join(OUTPUT_DIR, 'test'),\n",
    "                                             shuffle=True,\n",
    "                                             color_mode='rgb',\n",
    "                                             image_size=(image_h, image_w),\n",
    "                                             subset=None,\n",
    "                                             seed=0)\n",
    "\n",
    "# dataset validation spectrogram\n",
    "#validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "#                                             batch_size=batch_size,\n",
    "#                                             validation_split=0.2,\n",
    "#                                             directory=os.path.join(OUTPUT_DIR, 'audio-spectrograms'),\n",
    "#                                             shuffle=True,\n",
    "#                                             color_mode='rgb',\n",
    "#                                             image_size=(image_h, image_w),\n",
    "#                                             subset=\"validation\",\n",
    "#                                             seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-01-19T09:27:53.328865Z",
     "iopub.status.idle": "2022-01-19T09:27:53.329247Z",
     "shell.execute_reply": "2022-01-19T09:27:53.329108Z",
     "shell.execute_reply.started": "2022-01-19T09:27:53.329092Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare(ds):\n",
    "    # Define our one transformation\n",
    "    rescale = tf.keras.Sequential([tf.keras.layers.experimental.preprocessing.Rescaling(1./255)])\n",
    "    ds = ds.map(lambda x, y: (rescale(x, training=True), y))\n",
    "    return ds\n",
    "\n",
    "training_dataset = prepare(training_dataset)\n",
    "validation_dataset = prepare(validation_dataset)\n",
    "test_dataset = prepare(test_dataset)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-01-19T09:27:53.329994Z",
     "iopub.status.idle": "2022-01-19T09:27:53.331385Z",
     "shell.execute_reply": "2022-01-19T09:27:53.33107Z",
     "shell.execute_reply.started": "2022-01-19T09:27:53.331027Z"
    }
   },
   "outputs": [],
   "source": [
    "model=tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=(image_h,image_w,num_channel)))\n",
    "#convolution layer\n",
    "model.add(tf.keras.layers.Conv2D(32,3,strides=2,padding='same',activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "#pooling layer\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "#fully connected layer\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu',kernel_regularizer='l1'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "#output layer\n",
    "model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "checkpoint_filepath = 'weights.{epoch:02d}-{val_accuracy:.2f}.h5'\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "history=model.fit(training_dataset, epochs=10, validation_data=validation_dataset,callbacks=[model_checkpoint_callback])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## todo\n",
    "- fix crappy spectrograms\n",
    "- back to features, do feature importance to learn what factors are important\n",
    "- also look at other efforts (kaggle)\n",
    "- Also look at other datasets (respiratory)\n",
    "- Investigate training loss and validation loss. How do you know it's not the network\n",
    "\n",
    "- Use gans\n",
    "\n",
    "- If the augmentation improves the result, then that means the problem was I needed more data all along, or there is something about the old way of spectrograms (with axes and ticks?). Still worth repeating with better spectrograms\n",
    "\n",
    "- Look up what others have done and whether it is better\n",
    "\n",
    "- use pylint\n",
    "\n",
    "\n",
    "\n",
    "Spectrograms (surfboard):\n",
    "\n",
    "from surfboard.sound import Waveform\n",
    "from surfboard.feature_extraction import extract_features\n",
    "\n",
    "sound = Waveform(path='/path/to/audio.wav')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-01-19T09:27:53.333017Z",
     "iopub.status.idle": "2022-01-19T09:27:53.333672Z",
     "shell.execute_reply": "2022-01-19T09:27:53.333487Z",
     "shell.execute_reply.started": "2022-01-19T09:27:53.333461Z"
    }
   },
   "outputs": [],
   "source": [
    "#loss graph for training and validation\n",
    "history1=history.history\n",
    "loss=history1['loss']\n",
    "values=history1['val_loss']\n",
    "epochs=range(1,len(loss)+1)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(epochs, loss, 'bo', label=\"Training loss\")\n",
    "plt.plot(epochs, values, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-01-19T09:27:53.334778Z",
     "iopub.status.idle": "2022-01-19T09:27:53.335115Z",
     "shell.execute_reply": "2022-01-19T09:27:53.334943Z",
     "shell.execute_reply.started": "2022-01-19T09:27:53.334922Z"
    }
   },
   "outputs": [],
   "source": [
    "final_loss, final_acc=model.evaluate(test_dataset, verbose=0)\n",
    "print(\"Final loss: {0:.6f}\\n Final accuracy: {1:.6f}\".format(final_loss, final_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 585.852,
   "position": {
    "height": "40px",
    "left": "738px",
    "right": "20px",
    "top": "104px",
    "width": "477px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
