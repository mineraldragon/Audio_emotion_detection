{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-02-28T19:07:35.002231Z",
     "iopub.status.busy": "2022-02-28T19:07:35.001693Z",
     "iopub.status.idle": "2022-02-28T19:07:44.635043Z",
     "shell.execute_reply": "2022-02-28T19:07:44.633853Z",
     "shell.execute_reply.started": "2022-02-28T19:07:35.002123Z"
    }
   },
   "outputs": [],
   "source": [
    "#This notebook was originally copied from https://www.kaggle.com/samiaimad/ai-uni-project. \n",
    "#This copy is to make changes of my own \n",
    "\n",
    "#using virtual environment Voice\n",
    "\n",
    "#Rogier Landman\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# librosa is a Python library for analyzing audio and music.\n",
    "import librosa\n",
    "import librosa.display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# to play the audio files\n",
    "from IPython.display import Audio\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import wave\n",
    "import pylab\n",
    "from pathlib import Path\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import pylab\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPool2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Assigning Data Paths to Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T19:08:16.515756Z",
     "iopub.status.busy": "2022-02-28T19:08:16.515429Z",
     "iopub.status.idle": "2022-02-28T19:08:16.521288Z",
     "shell.execute_reply": "2022-02-28T19:08:16.520485Z",
     "shell.execute_reply.started": "2022-02-28T19:08:16.515722Z"
    }
   },
   "outputs": [],
   "source": [
    "Ravdess = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\"\n",
    "CremaD = \"/kaggle/input/cremad/AudioWAV/\"\n",
    "Tess = \"/kaggle/input/toronto-emotional-speech-set-tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data/\"\n",
    "Savee = \"/kaggle/input/savee-database/AudioData\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Audio Classification from the ravdess-emotional-speech-audio data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T19:08:24.505542Z",
     "iopub.status.busy": "2022-02-28T19:08:24.505217Z",
     "iopub.status.idle": "2022-02-28T19:08:25.248968Z",
     "shell.execute_reply": "2022-02-28T19:08:25.247931Z",
     "shell.execute_reply.started": "2022-02-28T19:08:24.505510Z"
    }
   },
   "outputs": [],
   "source": [
    "ravdess=os.listdir(Ravdess)\n",
    "\n",
    "emotion=[]\n",
    "path=[]\n",
    "gender=[]\n",
    "intensity=[]\n",
    "for i in ravdess:\n",
    "    actor=os.listdir(Ravdess+i)\n",
    "    for f in actor:\n",
    "        fsplit= f.split('.')[0]\n",
    "        fsplit=fsplit.split('-')\n",
    "        emotion.append(int(fsplit[2]))\n",
    "        gend=int(fsplit[6])\n",
    "        if gend%2==0:\n",
    "            gender.append('Female')\n",
    "        else:\n",
    "            gender.append('Male')\n",
    "        intensity.append(int(fsplit[3]))\n",
    "        for sp in emotion:\n",
    "            if sp==\"1\":\n",
    "                intensity.append('Normal')\n",
    "        path.append(Ravdess + i + '/' + f)\n",
    "edf=pd.DataFrame(emotion, columns=['Emotions'])\n",
    "pdf=pd.DataFrame(path, columns=['Path'])\n",
    "gdf=pd.DataFrame(gender, columns=['Gender'])\n",
    "idf=pd.DataFrame(intensity, columns=['Intensity'])\n",
    "\n",
    "ravdess_df=pd.concat([edf, pdf, gdf, idf], axis=1)\n",
    "\n",
    "ravdess_df.Emotions.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\n",
    "ravdess_df.Intensity.replace({1:'Normal', 2:'Strong'}, inplace=True)\n",
    "ravdess_df.head()\n",
    "ravdess_df.isnull()\n",
    "ravdess_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Audio Classification from the cremad data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T19:08:41.078395Z",
     "iopub.status.busy": "2022-02-28T19:08:41.077158Z",
     "iopub.status.idle": "2022-02-28T19:08:41.865205Z",
     "shell.execute_reply": "2022-02-28T19:08:41.864178Z",
     "shell.execute_reply.started": "2022-02-28T19:08:41.078330Z"
    }
   },
   "outputs": [],
   "source": [
    "crema=os.listdir(CremaD)\n",
    "\n",
    "emotion=[]\n",
    "path=[]\n",
    "gender=[]\n",
    "intensity=[]\n",
    "for f in crema:\n",
    "    part2= f.split('.')[0]\n",
    "    part2=part2.split('_')\n",
    "    \n",
    "    if int(part2[0])%2==0:\n",
    "        gender.append('Female')\n",
    "    else:\n",
    "        gender.append('Male')\n",
    "        \n",
    "    if part2[2] == 'SAD':\n",
    "        emotion.append('sad')\n",
    "    elif part2[2] == 'ANG':\n",
    "        emotion.append('angry')\n",
    "    elif part2[2] == 'DIS':\n",
    "        emotion.append('disgust')\n",
    "    elif part2[2] == 'FEA':\n",
    "        emotion.append('fear')\n",
    "    elif part2[2] == 'HAP':\n",
    "        emotion.append('happy')\n",
    "    elif part2[2] == 'NEU':\n",
    "        emotion.append('neutral')\n",
    "    else:\n",
    "        emotion.append('Unknown')\n",
    "        \n",
    "    if part2[3]=='HI':\n",
    "        intensity.append('High')\n",
    "    elif part2[3]=='LO':\n",
    "        intensity.append('Low')\n",
    "    elif part2[3]=='MD':\n",
    "        intensity.append('Medium')\n",
    "    elif part2[3]=='XX':\n",
    "        intensity.append('Unspecified')\n",
    "        \n",
    "    path.append(CremaD+f)\n",
    "    \n",
    "edf=pd.DataFrame(emotion, columns=['Emotions'])\n",
    "pdf=pd.DataFrame(path, columns=['Path'])\n",
    "gdf=pd.DataFrame(gender, columns=['Gender'])\n",
    "idf=pd.DataFrame(intensity, columns=['Intensity'])\n",
    "\n",
    "crema_df=pd.concat([edf, pdf, gdf, idf], axis=1)\n",
    "crema_df.head()\n",
    "crema_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Audio Classification from the toronto-emotional-speech-set-tess data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T19:08:50.795402Z",
     "iopub.status.busy": "2022-02-28T19:08:50.794613Z",
     "iopub.status.idle": "2022-02-28T19:08:51.361293Z",
     "shell.execute_reply": "2022-02-28T19:08:51.360402Z",
     "shell.execute_reply.started": "2022-02-28T19:08:50.795351Z"
    }
   },
   "outputs": [],
   "source": [
    "tess=os.listdir(Tess)\n",
    "\n",
    "emotion=[]\n",
    "path=[]\n",
    "gender=[]\n",
    "intensity=[]\n",
    "\n",
    "for i in tess:\n",
    "    dir=os.listdir(Tess+i)\n",
    "    for f in dir:\n",
    "        fsplit= f.split('.')[0]\n",
    "        fsplit=fsplit.split('_')[2]\n",
    "        if fsplit=='ps':\n",
    "            emotion.append('surprise')\n",
    "        else:\n",
    "            emotion.append(fsplit)\n",
    "        gender.append('Female')\n",
    "        path.append(Tess+i+'/'+f)\n",
    "        intensity.append('Unspecified')\n",
    "        \n",
    "edf=pd.DataFrame(emotion, columns=['Emotions'])\n",
    "pdf=pd.DataFrame(path, columns=['Path'])\n",
    "gdf=pd.DataFrame(gender, columns=['Gender'])\n",
    "idf=pd.DataFrame(intensity, columns=['Intensity'])\n",
    "\n",
    "tess_df=pd.concat([edf, pdf, gdf, idf], axis=1)\n",
    "tess_df.head()\n",
    "tess_df.isnull()\n",
    "tess_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Audio Classification from Savee Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T19:09:04.987680Z",
     "iopub.status.busy": "2022-02-28T19:09:04.987379Z",
     "iopub.status.idle": "2022-02-28T19:09:05.093980Z",
     "shell.execute_reply": "2022-02-28T19:09:05.092873Z",
     "shell.execute_reply.started": "2022-02-28T19:09:04.987647Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "Savee = \"/kaggle/input/savee-database/AudioData\"\n",
    "savee = os.listdir(Savee)\n",
    "\n",
    "emotion=[]\n",
    "path=[]\n",
    "gender=[]\n",
    "intensity=[]\n",
    "\n",
    "for f in savee:\n",
    "    if(f!=\"Info.txt\"):\n",
    "        x= Savee+\"/\"+f\n",
    "        dir=os.listdir(x)\n",
    "    \n",
    "    for d in dir:\n",
    "        dsplit = d.split(\".\")[0]\n",
    "        llsplit=re.split('\\d+',dsplit[0])\n",
    "        #print(llsplit)\n",
    "\n",
    "        for l in llsplit:\n",
    "            elist=l\n",
    "            if elist=='a':\n",
    "                emotion.append('angry')\n",
    "            elif elist=='d':\n",
    "                emotion.append('disgust')\n",
    "            elif elist=='f':\n",
    "                emotion.append('fear')\n",
    "            elif elist=='h':\n",
    "                emotion.append('happy')\n",
    "            elif elist=='n':\n",
    "                emotion.append('neutral')\n",
    "            elif elist=='sa':\n",
    "                emotion.append('sad')\n",
    "            else:\n",
    "                emotion.append('surprise')\n",
    "        gender.append('Male')\n",
    "        path.append(Savee+\"/\"+f)\n",
    "        intensity.append('Unspecified')\n",
    "\n",
    "edf=pd.DataFrame(emotion, columns=['Emotions'])\n",
    "pdf=pd.DataFrame(path, columns=['Path'])\n",
    "gdf=pd.DataFrame(gender, columns=['Gender'])\n",
    "idf=pd.DataFrame(intensity, columns=['Intensity'])\n",
    "\n",
    "savee_df=pd.concat([edf, pdf, gdf, idf], axis=1)\n",
    "#savee_df.head()\n",
    "savee_df.isnull()\n",
    "savee_df.info()\n",
    "savee_df.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Combining DataFrames (ravdess_df, crema_df,tess_df, savee_df)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T19:09:26.339795Z",
     "iopub.status.busy": "2022-02-28T19:09:26.339380Z",
     "iopub.status.idle": "2022-02-28T19:09:26.372158Z",
     "shell.execute_reply": "2022-02-28T19:09:26.370986Z",
     "shell.execute_reply.started": "2022-02-28T19:09:26.339761Z"
    }
   },
   "outputs": [],
   "source": [
    "Data_Combined = pd.concat([ravdess_df, crema_df,tess_df, savee_df], axis = 0)\n",
    "Data_Combined.isnull()\n",
    "Data_Combined.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T19:09:56.038650Z",
     "iopub.status.busy": "2022-02-28T19:09:56.038276Z",
     "iopub.status.idle": "2022-02-28T19:10:02.462663Z",
     "shell.execute_reply": "2022-02-28T19:10:02.461132Z",
     "shell.execute_reply.started": "2022-02-28T19:09:56.038611Z"
    }
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "flag = 0\n",
    "\n",
    "while (flag != 100): \n",
    "   \n",
    "    audio = wave.open(Data_Combined.iloc[flag, 1], 'r')\n",
    "    flag = flag + 1 \n",
    "    y= Data_Combined.iloc[flag, 0]\n",
    "    \n",
    "   \n",
    "    if (y not in x):\n",
    "        signal = np.frombuffer(audio.readframes(-1), dtype=np.int16)\n",
    "        plt.figure(figsize=(12,12))\n",
    "        plot1 = plt.subplot(211)\n",
    "        plot1.set_title(Data_Combined.iloc[flag, 0])\n",
    "        plot1.plot(signal)\n",
    "        plot1.set_xlabel('time * Number of samples')\n",
    "        plot1.set_ylabel('energy')\n",
    "\n",
    "        plot2 = plt.subplot(212)\n",
    "        plot2.specgram(signal, NFFT=1024, Fs=12281, noverlap=900)\n",
    "        plot2.set_xlabel('Time')\n",
    "        plot2.set_ylabel('Frequency')\n",
    "        \n",
    "        x.append(y)\n",
    "    \n",
    "    if (len(x)==8):\n",
    "        flag=100\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T19:10:33.779416Z",
     "iopub.status.busy": "2022-02-28T19:10:33.779079Z",
     "iopub.status.idle": "2022-02-28T19:10:33.785429Z",
     "shell.execute_reply": "2022-02-28T19:10:33.784016Z",
     "shell.execute_reply.started": "2022-02-28T19:10:33.779382Z"
    }
   },
   "outputs": [],
   "source": [
    "def stretch(data, rate=0.8):\n",
    "    return librosa.effects.time_stretch(data, rate)\n",
    "#x , sr = librosa.load(stretch(audio_data))\n",
    "#ya = stretch(x)\n",
    "#plt.figure(figsize=(14,4))\n",
    "#librosa.display.waveplot(y=ya, sr=sr)\n",
    "#Audio(ya, rate=sr)\n",
    "\n",
    "#SNR = 20*log(RMS_s/RMS_n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T19:10:39.754942Z",
     "iopub.status.busy": "2022-02-28T19:10:39.754615Z",
     "iopub.status.idle": "2022-02-28T19:10:40.160811Z",
     "shell.execute_reply": "2022-02-28T19:10:40.159129Z",
     "shell.execute_reply.started": "2022-02-28T19:10:39.754910Z"
    }
   },
   "outputs": [],
   "source": [
    "def white_noise(data,SNR):\n",
    "    RMS_s=math.sqrt(np.mean(data**2))\n",
    "    RMS_n=math.sqrt(RMS_s**2/(pow(10,SNR/10)))\n",
    "    STD_n=RMS_n\n",
    "    noise=np.random.normal(0, STD_n, data.shape[0])\n",
    "    data_noise=noise+data\n",
    "    return data_noise \n",
    "    \n",
    "\n",
    "x , sr = librosa.load(audio_data)\n",
    "ya = white_noise(x,SNR=10)\n",
    "plt.figure(figsize=(14,4))\n",
    "librosa.display.waveplot(y=ya, sr=sr)\n",
    "Audio(ya, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-01-19T09:27:53.322418Z",
     "iopub.status.idle": "2022-01-19T09:27:53.323563Z",
     "shell.execute_reply": "2022-01-19T09:27:53.323314Z",
     "shell.execute_reply.started": "2022-01-19T09:27:53.323255Z"
    }
   },
   "outputs": [],
   "source": [
    "def shifting_time(data, sampling_rate, shift_max, shift_direction):\n",
    "    shift = np.random.randint(sampling_rate * shift_max)\n",
    "    if shift_direction == 'right':\n",
    "        shift = -shift\n",
    "    elif self.shift_direction == 'both':\n",
    "        direction = np.random.randint(0, 2)\n",
    "        if direction == 1:\n",
    "            shift = -shift\n",
    "    augmented_data = np.roll(data, shift)\n",
    "    # Set to silence for heading/ tailing\n",
    "    if shift > 0:\n",
    "        augmented_data[:shift] = 0\n",
    "    else:\n",
    "        augmented_data[shift:] = 0\n",
    "    return augmented_data\n",
    "\n",
    "x , sr = librosa.load(audio_data)\n",
    "ya = shifting_time(x,sr,shift_max=1.5,shift_direction='right')\n",
    "plt.figure(figsize=(14,4))\n",
    "librosa.display.waveplot(y=ya, sr=sr)\n",
    "Audio(ya, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-01-19T09:27:53.324873Z",
     "iopub.status.idle": "2022-01-19T09:27:53.325416Z",
     "shell.execute_reply": "2022-01-19T09:27:53.325215Z",
     "shell.execute_reply.started": "2022-01-19T09:27:53.325194Z"
    }
   },
   "outputs": [],
   "source": [
    "def pitch(data, sampling_rate, pitch_factor=0.7):\n",
    "    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)\n",
    "x , sr = librosa.load(audio_data)\n",
    "ya = pitch(x,sr,pitch_factor=0.5)\n",
    "plt.figure(figsize=(14,4))\n",
    "librosa.display.waveplot(y=ya, sr=sr)\n",
    "Audio(ya, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Creating Spectograms images from Audio files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T19:12:01.018064Z",
     "iopub.status.busy": "2022-02-28T19:12:01.017775Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pylab\n",
    "\n",
    "output_path=\"./\"\n",
    "if  not os.path.exists(os.path.join(output_path, 'audio-spectrograms')):\n",
    "    os.mkdir(os.path.join(output_path, 'audio-spectrograms'))\n",
    "    \n",
    "#function to get sound and frame rate info\n",
    "def get_audio_info(audio_path):\n",
    "    wav = wave.open(audio_path, 'r')\n",
    "    frames = wav.readframes(-1)\n",
    "    audio_info = pylab.frombuffer(frames, 'int16')\n",
    "    frame_rate = wav.getframerate()\n",
    "    wav.close()\n",
    "    return audio_info, frame_rate\n",
    "\n",
    "\n",
    "\n",
    "for f in range (len(Data_Combined)):\n",
    "        file_Path = f'{Data_Combined.iloc[f, 1]}'\n",
    "        #print (file_path)\n",
    "        file_stem = Path(file_Path).stem\n",
    "        #print (file_stem)\n",
    "        target_dir = f'class_{Data_Combined.iloc[f, 0]}'\n",
    "        #print (target_dir)\n",
    "        dist_dir = os.path.join(os.path.join(output_path, 'audio-spectrograms'), target_dir)\n",
    "        #print (dist_dir)\n",
    "        # ex: ./audio-spectrograms/class_fear\n",
    "        file_dist_path = os.path.join(dist_dir, file_stem)\n",
    "        #ex: ./audio-spectrograms/class_fear/03-01-06-02-02-01-10\n",
    "        \n",
    "    \n",
    "        if not os.path.exists(file_dist_path + '.png'):\n",
    "            if not os.path.exists(dist_dir):\n",
    "                os.mkdir(dist_dir)\n",
    "            file_stem = Path(file_Path).stem\n",
    "            audio_info, frame_rate = get_audio_info(file_Path)\n",
    "            pylab.specgram(audio_info, Fs=frame_rate)\n",
    "            pylab.savefig(f'{file_dist_path}.png')\n",
    "            pylab.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-01-19T09:27:53.327644Z",
     "iopub.status.idle": "2022-01-19T09:27:53.328039Z",
     "shell.execute_reply": "2022-01-19T09:27:53.327885Z",
     "shell.execute_reply.started": "2022-01-19T09:27:53.327869Z"
    }
   },
   "outputs": [],
   "source": [
    "# Declare constants\n",
    "image_h = 256\n",
    "image_w = 256\n",
    "batch_size = 32\n",
    "num_channel = 3\n",
    "num_classes = 10\n",
    "OUTPUT_DIR = '/kaggle/working/'\n",
    "\n",
    "# dataset training spectrograms\n",
    "training_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "                                             batch_size=batch_size,\n",
    "                                             validation_split=0.2,\n",
    "                                             directory=os.path.join(OUTPUT_DIR, 'audio-spectrograms'),\n",
    "                                             shuffle=True,\n",
    "                                             color_mode='rgb',\n",
    "                                             image_size=(image_h, image_w),\n",
    "                                             subset=\"training\",\n",
    "                                             seed=0)\n",
    "\n",
    "# dataset validation spectrogram\n",
    "validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "                                             batch_size=batch_size,\n",
    "                                             validation_split=0.2,\n",
    "                                             directory=os.path.join(OUTPUT_DIR, 'audio-spectrograms'),\n",
    "                                             shuffle=True,\n",
    "                                             color_mode='rgb',\n",
    "                                             image_size=(image_h, image_w),\n",
    "                                             subset=\"validation\",\n",
    "                                             seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-01-19T09:27:53.328865Z",
     "iopub.status.idle": "2022-01-19T09:27:53.329247Z",
     "shell.execute_reply": "2022-01-19T09:27:53.329108Z",
     "shell.execute_reply.started": "2022-01-19T09:27:53.329092Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare(ds):\n",
    "    # Define our one transformation\n",
    "    rescale = tf.keras.Sequential([tf.keras.layers.experimental.preprocessing.Rescaling(1./255)])\n",
    "    ds = ds.map(lambda x, y: (rescale(x, training=True), y))\n",
    "    return ds\n",
    "\n",
    "training_dataset = prepare(training_dataset)\n",
    "validation_dataset = prepare(validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-01-19T09:27:53.329994Z",
     "iopub.status.idle": "2022-01-19T09:27:53.331385Z",
     "shell.execute_reply": "2022-01-19T09:27:53.33107Z",
     "shell.execute_reply.started": "2022-01-19T09:27:53.331027Z"
    }
   },
   "outputs": [],
   "source": [
    "model=tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=(image_h,image_w,num_channel)))\n",
    "#convolution layer\n",
    "model.add(tf.keras.layers.Conv2D(32,3,strides=2,padding='same',activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "#pooling layer\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "#fully connected layer\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "#output layer\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.RMSprop(),metrics=['accuracy'],)\n",
    "\n",
    "history=model.fit(training_dataset, epoch=10, validation=validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-01-19T09:27:53.333017Z",
     "iopub.status.idle": "2022-01-19T09:27:53.333672Z",
     "shell.execute_reply": "2022-01-19T09:27:53.333487Z",
     "shell.execute_reply.started": "2022-01-19T09:27:53.333461Z"
    }
   },
   "outputs": [],
   "source": [
    "#loss graph for training and validation\n",
    "history=history.history\n",
    "loss=history['loss']\n",
    "values=history['value']\n",
    "epochs=range(1,len(loss)+1)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(epochs, loss, 'bo', label=\"Training loss\")\n",
    "plt.plot(epochs, values, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-01-19T09:27:53.334778Z",
     "iopub.status.idle": "2022-01-19T09:27:53.335115Z",
     "shell.execute_reply": "2022-01-19T09:27:53.334943Z",
     "shell.execute_reply.started": "2022-01-19T09:27:53.334922Z"
    }
   },
   "outputs": [],
   "source": [
    "final_loss, final_acc=model.evaluate(valid, verbose=0)\n",
    "print(\"Final loss: {0:.6f}\\n Final accuracy: {1:.6f}\".format(final_loss, final_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
