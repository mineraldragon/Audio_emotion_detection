{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport sys\n\n# librosa is a Python library for analyzing audio and music.\nimport librosa\nimport librosa.display\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\n\n# to play the audio files\nfrom IPython.display import Audio\n\nimport keras\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\nfrom keras.utils import np_utils\nfrom keras.callbacks import ModelCheckpoint\n\nimport wave\nimport pylab\nfrom pathlib import Path\nfrom scipy import signal\nfrom scipy.io import wavfile\nfrom sklearn.metrics import confusion_matrix\nimport itertools\nimport pylab\n\nimport matplotlib.pyplot as plt\nimport math\n\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import MaxPool2D\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Dense\n\nimport warnings\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-28T19:07:35.001693Z","iopub.execute_input":"2022-02-28T19:07:35.002231Z","iopub.status.idle":"2022-02-28T19:07:44.635043Z","shell.execute_reply.started":"2022-02-28T19:07:35.002123Z","shell.execute_reply":"2022-02-28T19:07:44.633853Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# **Assigning Data Paths to Variables**","metadata":{}},{"cell_type":"code","source":"Ravdess = \"/kaggle/input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\"\nCremaD = \"/kaggle/input/cremad/AudioWAV/\"\nTess = \"/kaggle/input/toronto-emotional-speech-set-tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data/\"\nSavee = \"/kaggle/input/savee-database/AudioData\"\n","metadata":{"execution":{"iopub.status.busy":"2022-02-28T19:08:16.515429Z","iopub.execute_input":"2022-02-28T19:08:16.515756Z","iopub.status.idle":"2022-02-28T19:08:16.521288Z","shell.execute_reply.started":"2022-02-28T19:08:16.515722Z","shell.execute_reply":"2022-02-28T19:08:16.520485Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# **Audio Classification from the ravdess-emotional-speech-audio data set**","metadata":{}},{"cell_type":"code","source":"ravdess=os.listdir(Ravdess)\n\nemotion=[]\npath=[]\ngender=[]\nintensity=[]\nfor i in ravdess:\n    actor=os.listdir(Ravdess+i)\n    for f in actor:\n        fsplit= f.split('.')[0]\n        fsplit=fsplit.split('-')\n        emotion.append(int(fsplit[2]))\n        gend=int(fsplit[6])\n        if gend%2==0:\n            gender.append('Female')\n        else:\n            gender.append('Male')\n        intensity.append(int(fsplit[3]))\n        for sp in emotion:\n            if sp==\"1\":\n                intensity.append('Normal')\n        path.append(Ravdess + i + '/' + f)\nedf=pd.DataFrame(emotion, columns=['Emotions'])\npdf=pd.DataFrame(path, columns=['Path'])\ngdf=pd.DataFrame(gender, columns=['Gender'])\nidf=pd.DataFrame(intensity, columns=['Intensity'])\n\nravdess_df=pd.concat([edf, pdf, gdf, idf], axis=1)\n\nravdess_df.Emotions.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\nravdess_df.Intensity.replace({1:'Normal', 2:'Strong'}, inplace=True)\nravdess_df.head()\nravdess_df.isnull()\nravdess_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-02-28T19:08:24.505217Z","iopub.execute_input":"2022-02-28T19:08:24.505542Z","iopub.status.idle":"2022-02-28T19:08:25.248968Z","shell.execute_reply.started":"2022-02-28T19:08:24.505510Z","shell.execute_reply":"2022-02-28T19:08:25.247931Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# **Audio Classification from the cremad data set**","metadata":{}},{"cell_type":"code","source":"crema=os.listdir(CremaD)\n\nemotion=[]\npath=[]\ngender=[]\nintensity=[]\nfor f in crema:\n    part2= f.split('.')[0]\n    part2=part2.split('_')\n    \n    if int(part2[0])%2==0:\n        gender.append('Female')\n    else:\n        gender.append('Male')\n        \n    if part2[2] == 'SAD':\n        emotion.append('sad')\n    elif part2[2] == 'ANG':\n        emotion.append('angry')\n    elif part2[2] == 'DIS':\n        emotion.append('disgust')\n    elif part2[2] == 'FEA':\n        emotion.append('fear')\n    elif part2[2] == 'HAP':\n        emotion.append('happy')\n    elif part2[2] == 'NEU':\n        emotion.append('neutral')\n    else:\n        emotion.append('Unknown')\n        \n    if part2[3]=='HI':\n        intensity.append('High')\n    elif part2[3]=='LO':\n        intensity.append('Low')\n    elif part2[3]=='MD':\n        intensity.append('Medium')\n    elif part2[3]=='XX':\n        intensity.append('Unspecified')\n        \n    path.append(CremaD+f)\n    \nedf=pd.DataFrame(emotion, columns=['Emotions'])\npdf=pd.DataFrame(path, columns=['Path'])\ngdf=pd.DataFrame(gender, columns=['Gender'])\nidf=pd.DataFrame(intensity, columns=['Intensity'])\n\ncrema_df=pd.concat([edf, pdf, gdf, idf], axis=1)\ncrema_df.head()\ncrema_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-02-28T19:08:41.077158Z","iopub.execute_input":"2022-02-28T19:08:41.078395Z","iopub.status.idle":"2022-02-28T19:08:41.865205Z","shell.execute_reply.started":"2022-02-28T19:08:41.078330Z","shell.execute_reply":"2022-02-28T19:08:41.864178Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# **Audio Classification from the toronto-emotional-speech-set-tess data set**","metadata":{}},{"cell_type":"code","source":"tess=os.listdir(Tess)\n\nemotion=[]\npath=[]\ngender=[]\nintensity=[]\n\nfor i in tess:\n    dir=os.listdir(Tess+i)\n    for f in dir:\n        fsplit= f.split('.')[0]\n        fsplit=fsplit.split('_')[2]\n        if fsplit=='ps':\n            emotion.append('surprise')\n        else:\n            emotion.append(fsplit)\n        gender.append('Female')\n        path.append(Tess+i+'/'+f)\n        intensity.append('Unspecified')\n        \nedf=pd.DataFrame(emotion, columns=['Emotions'])\npdf=pd.DataFrame(path, columns=['Path'])\ngdf=pd.DataFrame(gender, columns=['Gender'])\nidf=pd.DataFrame(intensity, columns=['Intensity'])\n\ntess_df=pd.concat([edf, pdf, gdf, idf], axis=1)\ntess_df.head()\ntess_df.isnull()\ntess_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-02-28T19:08:50.794613Z","iopub.execute_input":"2022-02-28T19:08:50.795402Z","iopub.status.idle":"2022-02-28T19:08:51.361293Z","shell.execute_reply.started":"2022-02-28T19:08:50.795351Z","shell.execute_reply":"2022-02-28T19:08:51.360402Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# **Audio Classification from Savee Dataset**","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nSavee = \"/kaggle/input/savee-database/AudioData\"\nsavee = os.listdir(Savee)\n\nemotion=[]\npath=[]\ngender=[]\nintensity=[]\n\nfor f in savee:\n    if(f!=\"Info.txt\"):\n        x= Savee+\"/\"+f\n        dir=os.listdir(x)\n    \n    for d in dir:\n        dsplit = d.split(\".\")[0]\n        llsplit=re.split('\\d+',dsplit[0])\n        #print(llsplit)\n\n        for l in llsplit:\n            elist=l\n            if elist=='a':\n                emotion.append('angry')\n            elif elist=='d':\n                emotion.append('disgust')\n            elif elist=='f':\n                emotion.append('fear')\n            elif elist=='h':\n                emotion.append('happy')\n            elif elist=='n':\n                emotion.append('neutral')\n            elif elist=='sa':\n                emotion.append('sad')\n            else:\n                emotion.append('surprise')\n        gender.append('Male')\n        path.append(Savee+\"/\"+f)\n        intensity.append('Unspecified')\n\nedf=pd.DataFrame(emotion, columns=['Emotions'])\npdf=pd.DataFrame(path, columns=['Path'])\ngdf=pd.DataFrame(gender, columns=['Gender'])\nidf=pd.DataFrame(intensity, columns=['Intensity'])\n\nsavee_df=pd.concat([edf, pdf, gdf, idf], axis=1)\n#savee_df.head()\nsavee_df.isnull()\nsavee_df.info()\nsavee_df.tail()\n","metadata":{"execution":{"iopub.status.busy":"2022-02-28T19:09:04.987379Z","iopub.execute_input":"2022-02-28T19:09:04.987680Z","iopub.status.idle":"2022-02-28T19:09:05.093980Z","shell.execute_reply.started":"2022-02-28T19:09:04.987647Z","shell.execute_reply":"2022-02-28T19:09:05.092873Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# **Combining DataFrames (ravdess_df, crema_df,tess_df, savee_df)**","metadata":{}},{"cell_type":"code","source":"Data_Combined = pd.concat([ravdess_df, crema_df,tess_df, savee_df], axis = 0)\nData_Combined.isnull()\nData_Combined.info()","metadata":{"execution":{"iopub.status.busy":"2022-02-28T19:09:26.339380Z","iopub.execute_input":"2022-02-28T19:09:26.339795Z","iopub.status.idle":"2022-02-28T19:09:26.372158Z","shell.execute_reply.started":"2022-02-28T19:09:26.339761Z","shell.execute_reply":"2022-02-28T19:09:26.370986Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# **Data Visualization**","metadata":{}},{"cell_type":"code","source":"x = []\nflag = 0\n\nwhile (flag != 100): \n   \n    audio = wave.open(Data_Combined.iloc[flag, 1], 'r')\n    flag = flag + 1 \n    y= Data_Combined.iloc[flag, 0]\n    \n   \n    if (y not in x):\n        signal = np.frombuffer(audio.readframes(-1), dtype=np.int16)\n        plt.figure(figsize=(12,12))\n        plot1 = plt.subplot(211)\n        plot1.set_title(Data_Combined.iloc[flag, 0])\n        plot1.plot(signal)\n        plot1.set_xlabel('time * Number of samples')\n        plot1.set_ylabel('energy')\n\n        plot2 = plt.subplot(212)\n        plot2.specgram(signal, NFFT=1024, Fs=12281, noverlap=900)\n        plot2.set_xlabel('Time')\n        plot2.set_ylabel('Frequency')\n        \n        x.append(y)\n    \n    if (len(x)==8):\n        flag=100\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-28T19:09:56.038276Z","iopub.execute_input":"2022-02-28T19:09:56.038650Z","iopub.status.idle":"2022-02-28T19:10:02.462663Z","shell.execute_reply.started":"2022-02-28T19:09:56.038611Z","shell.execute_reply":"2022-02-28T19:10:02.461132Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# **Data Augmentation**","metadata":{}},{"cell_type":"code","source":"def stretch(data, rate=0.8):\n    return librosa.effects.time_stretch(data, rate)\n#x , sr = librosa.load(stretch(audio_data))\n#ya = stretch(x)\n#plt.figure(figsize=(14,4))\n#librosa.display.waveplot(y=ya, sr=sr)\n#Audio(ya, rate=sr)\n\n#SNR = 20*log(RMS_s/RMS_n)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-28T19:10:33.779079Z","iopub.execute_input":"2022-02-28T19:10:33.779416Z","iopub.status.idle":"2022-02-28T19:10:33.785429Z","shell.execute_reply.started":"2022-02-28T19:10:33.779382Z","shell.execute_reply":"2022-02-28T19:10:33.784016Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def white_noise(data,SNR):\n    RMS_s=math.sqrt(np.mean(data**2))\n    RMS_n=math.sqrt(RMS_s**2/(pow(10,SNR/10)))\n    STD_n=RMS_n\n    noise=np.random.normal(0, STD_n, data.shape[0])\n    data_noise=noise+data\n    return data_noise \n    \n\nx , sr = librosa.load(audio_data)\nya = white_noise(x,SNR=10)\nplt.figure(figsize=(14,4))\nlibrosa.display.waveplot(y=ya, sr=sr)\nAudio(ya, rate=sr)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T19:10:39.754615Z","iopub.execute_input":"2022-02-28T19:10:39.754942Z","iopub.status.idle":"2022-02-28T19:10:40.160811Z","shell.execute_reply.started":"2022-02-28T19:10:39.754910Z","shell.execute_reply":"2022-02-28T19:10:40.159129Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def shifting_time(data, sampling_rate, shift_max, shift_direction):\n    shift = np.random.randint(sampling_rate * shift_max)\n    if shift_direction == 'right':\n        shift = -shift\n    elif self.shift_direction == 'both':\n        direction = np.random.randint(0, 2)\n        if direction == 1:\n            shift = -shift\n    augmented_data = np.roll(data, shift)\n    # Set to silence for heading/ tailing\n    if shift > 0:\n        augmented_data[:shift] = 0\n    else:\n        augmented_data[shift:] = 0\n    return augmented_data\n\nx , sr = librosa.load(audio_data)\nya = shifting_time(x,sr,shift_max=1.5,shift_direction='right')\nplt.figure(figsize=(14,4))\nlibrosa.display.waveplot(y=ya, sr=sr)\nAudio(ya, rate=sr)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T09:27:53.322418Z","iopub.status.idle":"2022-01-19T09:27:53.323563Z","shell.execute_reply.started":"2022-01-19T09:27:53.323255Z","shell.execute_reply":"2022-01-19T09:27:53.323314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pitch(data, sampling_rate, pitch_factor=0.7):\n    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)\nx , sr = librosa.load(audio_data)\nya = pitch(x,sr,pitch_factor=0.5)\nplt.figure(figsize=(14,4))\nlibrosa.display.waveplot(y=ya, sr=sr)\nAudio(ya, rate=sr)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T09:27:53.324873Z","iopub.status.idle":"2022-01-19T09:27:53.325416Z","shell.execute_reply.started":"2022-01-19T09:27:53.325194Z","shell.execute_reply":"2022-01-19T09:27:53.325215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Creating Spectograms images from Audio files**","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nimport pylab\n\noutput_path=\"./\"\nif  not os.path.exists(os.path.join(output_path, 'audio-spectrograms')):\n    os.mkdir(os.path.join(output_path, 'audio-spectrograms'))\n    \n#function to get sound and frame rate info\ndef get_audio_info(audio_path):\n    wav = wave.open(audio_path, 'r')\n    frames = wav.readframes(-1)\n    audio_info = pylab.frombuffer(frames, 'int16')\n    frame_rate = wav.getframerate()\n    wav.close()\n    return audio_info, frame_rate\n\n\n\nfor f in range (len(Data_Combined)):\n        file_Path = f'{Data_Combined.iloc[f, 1]}'\n        #print (file_path)\n        file_stem = Path(file_Path).stem\n        #print (file_stem)\n        target_dir = f'class_{Data_Combined.iloc[f, 0]}'\n        #print (target_dir)\n        dist_dir = os.path.join(os.path.join(output_path, 'audio-spectrograms'), target_dir)\n        #print (dist_dir)\n        # ex: ./audio-spectrograms/class_fear\n        file_dist_path = os.path.join(dist_dir, file_stem)\n        #ex: ./audio-spectrograms/class_fear/03-01-06-02-02-01-10\n        \n    \n        if not os.path.exists(file_dist_path + '.png'):\n            if not os.path.exists(dist_dir):\n                os.mkdir(dist_dir)\n            file_stem = Path(file_Path).stem\n            audio_info, frame_rate = get_audio_info(file_Path)\n            pylab.specgram(audio_info, Fs=frame_rate)\n            pylab.savefig(f'{file_dist_path}.png')\n            pylab.close()","metadata":{"execution":{"iopub.status.busy":"2022-02-28T19:12:01.017775Z","iopub.execute_input":"2022-02-28T19:12:01.018064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Preparation**","metadata":{}},{"cell_type":"code","source":"# Declare constants\nimage_h = 256\nimage_w = 256\nbatch_size = 32\nnum_channel = 3\nnum_classes = 10\nOUTPUT_DIR = '/kaggle/working/'\n\n# dataset training spectrograms\ntraining_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n                                             batch_size=batch_size,\n                                             validation_split=0.2,\n                                             directory=os.path.join(OUTPUT_DIR, 'audio-spectrograms'),\n                                             shuffle=True,\n                                             color_mode='rgb',\n                                             image_size=(image_h, image_w),\n                                             subset=\"training\",\n                                             seed=0)\n\n# dataset validation spectrogram\nvalidation_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n                                             batch_size=batch_size,\n                                             validation_split=0.2,\n                                             directory=os.path.join(OUTPUT_DIR, 'audio-spectrograms'),\n                                             shuffle=True,\n                                             color_mode='rgb',\n                                             image_size=(image_h, image_w),\n                                             subset=\"validation\",\n                                             seed=0)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T09:27:53.327644Z","iopub.status.idle":"2022-01-19T09:27:53.328039Z","shell.execute_reply.started":"2022-01-19T09:27:53.327869Z","shell.execute_reply":"2022-01-19T09:27:53.327885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare(ds):\n    # Define our one transformation\n    rescale = tf.keras.Sequential([tf.keras.layers.experimental.preprocessing.Rescaling(1./255)])\n    ds = ds.map(lambda x, y: (rescale(x, training=True), y))\n    return ds\n\ntraining_dataset = prepare(training_dataset)\nvalidation_dataset = prepare(validation_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T09:27:53.328865Z","iopub.status.idle":"2022-01-19T09:27:53.329247Z","shell.execute_reply.started":"2022-01-19T09:27:53.329092Z","shell.execute_reply":"2022-01-19T09:27:53.329108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Modeling**","metadata":{}},{"cell_type":"code","source":"model=tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Input(shape=(image_h,image_w,num_channel)))\n#convolution layer\nmodel.add(tf.keras.layers.Conv2D(32,3,strides=2,padding='same',activation='relu'))\nmodel.add(tf.keras.layers.BatchNormalization())\n#pooling layer\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\nmodel.add(tf.keras.layers.BatchNormalization())\n#fully connected layer\nmodel.add(tf.keras.layers.Flatten())\nmodel.add(tf.keras.layers.Dense(256, activation='relu'))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Dropout(0.5))\n#output layer\nmodel.add(tf.keras.layers.Dense(10, activation='softmax'))\n\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.RMSprop(),metrics=['accuracy'],)\n\nhistory=model.fit(training_dataset, epoch=10, validation=validation_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T09:27:53.329994Z","iopub.status.idle":"2022-01-19T09:27:53.331385Z","shell.execute_reply.started":"2022-01-19T09:27:53.331027Z","shell.execute_reply":"2022-01-19T09:27:53.33107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#loss graph for training and validation\nhistory=history.history\nloss=history['loss']\nvalues=history['value']\nepochs=range(1,len(loss)+1)\n\nplt.figure(figsize=(8,6))\nplt.plot(epochs, loss, 'bo', label=\"Training loss\")\nplt.plot(epochs, values, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-19T09:27:53.333017Z","iopub.status.idle":"2022-01-19T09:27:53.333672Z","shell.execute_reply.started":"2022-01-19T09:27:53.333461Z","shell.execute_reply":"2022-01-19T09:27:53.333487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_loss, final_acc=model.evaluate(valid, verbose=0)\nprint(\"Final loss: {0:.6f}\\n Final accuracy: {1:.6f}\".format(final_loss, final_acc))","metadata":{"execution":{"iopub.status.busy":"2022-01-19T09:27:53.334778Z","iopub.status.idle":"2022-01-19T09:27:53.335115Z","shell.execute_reply.started":"2022-01-19T09:27:53.334922Z","shell.execute_reply":"2022-01-19T09:27:53.334943Z"},"trusted":true},"execution_count":null,"outputs":[]}]}